{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suzan Verberne 17.10.2023 1\n",
    "Assignment 2: sequence labelling\n",
    "Text mining course\n",
    "This is a hand-in assignment for groups of two students. Send in via Brightspace before or on\n",
    "Tuesday November 7:\n",
    "• Submit your report as PDF and your python code as separate file. Don’t upload a zip file\n",
    "containing the PDF (the Python code might be zipped if it consists of multiple files).\n",
    "• Your report should not be longer than 3 pages.\n",
    "• Do not copy text from sources (other groups, web pages, generative models such as\n",
    "chatGPT). Turnitin is enabled and a large overlap will be reported to the Board of Examiners.\n",
    "Goals of this assignment\n",
    "• You can pre-process existing annotated text data into the data structure that you need for\n",
    "classifier learning.\n",
    "• You can perform hyperparameter optimization.\n",
    "• You can perform a sequence labelling task with annotated data in Huggingface.\n",
    "• You can evaluate sequence labelling with the suitable evaluation metrics.\n",
    "Preliminaries\n",
    "• You have followed the Huggingface tutorial on token classification\n",
    "https://huggingface.co/learn/nlp-course/chapter7/2 and its preliminaries (exercises week 6\n",
    "and 7).\n",
    "• You have all the required Python packages installed and Python 3.10.\n",
    "We are going to train an NER classifier for the task “Emerging and Rare entity recognition” from the\n",
    "Workshop on Noisy User-generated Text (W-NUT). The description of the task can be found at\n",
    "https://noisy-text.github.io/2017/emerging-rare-entities.html (I put the data itself on Brightspace)\n",
    "Tasks\n",
    "1. Download W-NUT_data.zip from the Brightspace assignment and unzip the directory. It\n",
    "contains 3 IOB files: wnut17train.conll (train), emerging.dev.conll (dev),\n",
    "emerging.test.annotated (test).\n",
    "2. Convert the IOB data to the correct data structure for token classification in Huggingface\n",
    "(words and labels like the conll2023 data in the tutorial) and align the labels with the tokens.\n",
    "Note that since you are working with a custom dataset, the data conversion is a necessary\n",
    "step for using the Huggingface training function.\n",
    "Suzan Verberne 17.10.2023 2\n",
    "3. Set up the evaluation correctly for the W-NUT test set, following the tutorial.\n",
    "4. Fine-tune a model with the default hyperparameter settings on the train set and evaluate\n",
    "the model on the test set. These are your baseline results.\n",
    "5. Set up hyperparameter optimization with the AdamW optimizer. During optimization, use\n",
    "the dev set as validation. After the model has been optimized, evaluate the result on the test\n",
    "set.\n",
    "6. Extend the evaluation function so that it shows the Precision, Recall and F-score for each of\n",
    "the entity types (person, location, etc.) on the test set. Include the metrics for the B-label of\n",
    "the entity type, the I-label, and the full entities. Look up the definitions of macro- and micro-\n",
    "average scores and compute the macro- and micro average F1 scores over all entities.\n",
    "Write a report of at most 3 pages in which you:\n",
    "• describe the task and the data (give a few statistics. What are the entity types?) and briefly\n",
    "describe two challenges of the task and the data.\n",
    "• show your results:\n",
    "o a results table with both the baseline results and the results after hyperparameter\n",
    "optimization (do not report results on the dev set, only on the test set): a table with\n",
    "Precision, Recall, F-score for both settings.\n",
    "o a table with the results after hyperparameter optimization for the different entity\n",
    "types (Precision, Recall, F-score for B, I, and the full entities), and the macro- and\n",
    "micro F1 scores.\n",
    "• write brief conclusions. Address the following questions:\n",
    "o What is the effect of hyperparameter optimization on the quality of the model?\n",
    "o What does the difference between scores for different entity types tell you?\n",
    "o Where does the difference between macro- and micro-averaged F1 scores come\n",
    "from?\n",
    "Grading\n",
    "Maximum 2 points for each of the following criteria:\n",
    "• General: length correct (2-3 pages) and proper writing + formatting\n",
    "• Description of the task and the data, with description of 2 challenges\n",
    "• Baseline results with default hyperparameter settings and results with optimized\n",
    "hyperparameter settings\n",
    "• Results after hyperparameter optimization for the different entity types\n",
    "• Sensible conclusions, briefly addressing the questions listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\n",
    "    \"O\",\n",
    "    \"B-corporation\",\n",
    "    \"I-corporation\",\n",
    "    \"B-creative-work\",\n",
    "    \"I-creative-work\",\n",
    "    \"B-group\",\n",
    "    \"I-group\",\n",
    "    \"B-location\",\n",
    "    \"I-location\",\n",
    "    \"B-person\",\n",
    "    \"I-person\",\n",
    "    \"B-product\",\n",
    "    \"I-product\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, valid, test = 'wnut17train.conll', 'emerging.dev.conll', 'emerging.test.annotated'\n",
    "path = 'W-NUT_data'\n",
    "\n",
    "raw_datasets = {}\n",
    "for file, name in zip([train, valid, test], ['train', 'validation', 'test']):\n",
    "    id = 0\n",
    "    raw_datasets[name] = {'id': [], 'tokens': [], 'ner_tags': []}\n",
    "    with open(f'{path}/{file}', 'r') as f:\n",
    "        tokens, ner_tags = [], []\n",
    "        for line in f:\n",
    "            try:\n",
    "                token, ner_tag = line.split()\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(names.index(ner_tag))\n",
    "            except:\n",
    "                raw_datasets[name]['id'].append([id for _ in range(len(tokens))])\n",
    "                raw_datasets[name]['tokens'].append(tokens)\n",
    "                raw_datasets[name]['ner_tags'].append(ner_tags)\n",
    "                id += 1\n",
    "                tokens, ner_tags = [], []\n",
    "    raw_datasets[name] = datasets.Dataset.from_dict(raw_datasets[name])\n",
    "\n",
    "raw_datasets = datasets.DatasetDict(raw_datasets)\n",
    "display(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b6cf3d6ccf4442b25d1e8363bf175a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da20773d13e49f099973860d3963080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b20451e69f24d198fc7b38c16ffbd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Ruben\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae443c7dcc24d998b79c0dd56af54c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a9cb2ee78045228bb6192829099658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34612947702407837, 'eval_precision': 0.5754527162977867, 'eval_recall': 0.34210526315789475, 'eval_f1': 0.4291072768192048, 'eval_accuracy': 0.9137275889751137, 'eval_runtime': 3.6261, 'eval_samples_per_second': 278.261, 'eval_steps_per_second': 35.024, 'epoch': 1.0}\n",
      "{'loss': 0.2052, 'learning_rate': 1.215686274509804e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d075239b3664cb6b7761a1b645bd4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37926626205444336, 'eval_precision': 0.5973597359735974, 'eval_recall': 0.43301435406698563, 'eval_f1': 0.5020804438280165, 'eval_accuracy': 0.9232004281509232, 'eval_runtime': 3.6961, 'eval_samples_per_second': 272.992, 'eval_steps_per_second': 34.361, 'epoch': 2.0}\n",
      "{'loss': 0.0836, 'learning_rate': 4.313725490196079e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf2ac75baab4661a3f3496ef135a85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3744765818119049, 'eval_precision': 0.5727002967359051, 'eval_recall': 0.46172248803827753, 'eval_f1': 0.5112582781456952, 'eval_accuracy': 0.9256622959593257, 'eval_runtime': 3.6361, 'eval_samples_per_second': 277.495, 'eval_steps_per_second': 34.927, 'epoch': 3.0}\n",
      "{'train_runtime': 254.9357, 'train_samples_per_second': 39.939, 'train_steps_per_second': 5.001, 'train_loss': 0.12712095522413067, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1275, training_loss=0.12712095522413067, metrics={'train_runtime': 254.9357, 'train_samples_per_second': 39.939, 'train_steps_per_second': 5.001, 'train_loss': 0.12712095522413067, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rubinho/bert-finetuned-ner/tree/main/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(all_metrics):\n",
    "    print(\"\\\\begin table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\begin{tabular}{lrrrr}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\" & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1} & \\\\textbf{Number} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    for key, value in all_metrics.items():\n",
    "        if \"overall\" not in key:\n",
    "            print(f\"{key.title()} & {value['precision']:.2f} & {value['recall']:.2f} & {value['f1']:.2f} & {value['number']} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    print(\" & \\\\textbf{Overall precision} & \\\\textbf{Overall recall} & \\\\textbf{Overall F1} & \\\\textbf{Overall accuracy} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    print(f\" & {all_metrics['overall_precision']:.2f} & {all_metrics['overall_recall']:.2f} & {all_metrics['overall_f1']:.2f} & {all_metrics['overall_accuracy']:.2f} \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\caption{Results of the NER model on the test set.}\")\n",
    "    print(\"\\\\label{tab:ner-results}\")\n",
    "    print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c0cc213d6647ad8261798902a0bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1} & \\textbf{Number} \\\\\n",
      "\\midrule\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PredictionOutput' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ruben\\Documents\\Studie\\Msc CS\\Text Mining\\txtmining_a2\\txtmin_a2.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m metrics \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mpredict(tokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y136sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m TFcompute_metrics(metrics);\n",
      "\u001b[1;32mc:\\Users\\Ruben\\Documents\\Studie\\Msc CS\\Text Mining\\txtmining_a2\\txtmin_a2.ipynb Cell 29\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y136sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m & \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtextbf\u001b[39m\u001b[39m{Precision}\u001b[39;00m\u001b[39m & \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtextbf\u001b[39m\u001b[39m{Recall}\u001b[39;00m\u001b[39m & \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtextbf\u001b[39m\u001b[39m{F1}\u001b[39;00m\u001b[39m & \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtextbf\u001b[39m\u001b[39m{Number}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y136sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmidrule\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y136sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m all_metrics\u001b[39m.\u001b[39;49mitems():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y136sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39moverall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m key:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y136sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m.\u001b[39mtitle()\u001b[39m}\u001b[39;00m\u001b[39m & \u001b[39m\u001b[39m{\u001b[39;00mvalue[\u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m & \u001b[39m\u001b[39m{\u001b[39;00mvalue[\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m & \u001b[39m\u001b[39m{\u001b[39;00mvalue[\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m & \u001b[39m\u001b[39m{\u001b[39;00mvalue[\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PredictionOutput' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "\n",
    "\n",
    "compute_metrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n",
    "        \"beta_1\": trial.suggest_float(\"beta_1\", 0.9, 0.999),\n",
    "        \"beta_2\": trial.suggest_float(\"beta_2\", 0.9, 0.999),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True),\n",
    "    }\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate w/o hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ruben\\Documents\\Studie\\Msc CS\\Text Mining\\txtmining_a2\\txtmin_a2.ipynb Cell 29\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m all_predictions \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y102sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tf_test_dataset:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_on_batch(batch)[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ruben/Documents/Studie/Msc%20CS/Text%20Mining/txtmining_a2/txtmin_a2.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "for batch in tf_test_dataset:\n",
    "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for predicted_idx, label_idx in zip(prediction, label):\n",
    "            if label_idx == -100:\n",
    "                continue\n",
    "            all_predictions.append(names[predicted_idx])\n",
    "            all_labels.append(names[label_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFcompute_metrics(all_metrics):\n",
    "    print(\"\\\\begin table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\begin{tabular}{lrrrr}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\" & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1} & \\\\textbf{Number} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    for key, value in all_metrics.items():\n",
    "        if \"overall\" not in key:\n",
    "            print(f\"{key.title()} & {value['precision']:.2f} & {value['recall']:.2f} & {value['f1']:.2f} & {value['number']} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    print(\" & \\\\textbf{Overall precision} & \\\\textbf{Overall recall} & \\\\textbf{Overall F1} & \\\\textbf{Overall accuracy} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    print(f\" & {all_metrics['overall_precision']:.2f} & {all_metrics['overall_recall']:.2f} & {all_metrics['overall_f1']:.2f} & {all_metrics['overall_accuracy']:.2f} \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\caption{Results of the NER model on the test set.}\")\n",
    "    print(\"\\\\label{tab:ner-results}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "TFcompute_metrics(all_predictions, all_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=0.001,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.004\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n",
    "        \"beta_1\": trial.suggest_float(\"beta_1\", 0.9, 0.999),\n",
    "        \"beta_2\": trial.suggest_float(\"beta_2\", 0.9, 0.999),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tf_train_dataset,\n",
    "    eval_dataset=tf_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
