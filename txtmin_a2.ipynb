{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suzan Verberne 17.10.2023 1\n",
    "Assignment 2: sequence labelling\n",
    "Text mining course\n",
    "This is a hand-in assignment for groups of two students. Send in via Brightspace before or on\n",
    "Tuesday November 7:\n",
    "• Submit your report as PDF and your python code as separate file. Don’t upload a zip file\n",
    "containing the PDF (the Python code might be zipped if it consists of multiple files).\n",
    "• Your report should not be longer than 3 pages.\n",
    "• Do not copy text from sources (other groups, web pages, generative models such as\n",
    "chatGPT). Turnitin is enabled and a large overlap will be reported to the Board of Examiners.\n",
    "Goals of this assignment\n",
    "• You can pre-process existing annotated text data into the data structure that you need for\n",
    "classifier learning.\n",
    "• You can perform hyperparameter optimization.\n",
    "• You can perform a sequence labelling task with annotated data in Huggingface.\n",
    "• You can evaluate sequence labelling with the suitable evaluation metrics.\n",
    "Preliminaries\n",
    "• You have followed the Huggingface tutorial on token classification\n",
    "https://huggingface.co/learn/nlp-course/chapter7/2 and its preliminaries (exercises week 6\n",
    "and 7).\n",
    "• You have all the required Python packages installed and Python 3.10.\n",
    "We are going to train an NER classifier for the task “Emerging and Rare entity recognition” from the\n",
    "Workshop on Noisy User-generated Text (W-NUT). The description of the task can be found at\n",
    "https://noisy-text.github.io/2017/emerging-rare-entities.html (I put the data itself on Brightspace)\n",
    "Tasks\n",
    "1. Download W-NUT_data.zip from the Brightspace assignment and unzip the directory. It\n",
    "contains 3 IOB files: wnut17train.conll (train), emerging.dev.conll (dev),\n",
    "emerging.test.annotated (test).\n",
    "2. Convert the IOB data to the correct data structure for token classification in Huggingface\n",
    "(words and labels like the conll2023 data in the tutorial) and align the labels with the tokens.\n",
    "Note that since you are working with a custom dataset, the data conversion is a necessary\n",
    "step for using the Huggingface training function.\n",
    "Suzan Verberne 17.10.2023 2\n",
    "3. Set up the evaluation correctly for the W-NUT test set, following the tutorial.\n",
    "4. Fine-tune a model with the default hyperparameter settings on the train set and evaluate\n",
    "the model on the test set. These are your baseline results.\n",
    "5. Set up hyperparameter optimization with the AdamW optimizer. During optimization, use\n",
    "the dev set as validation. After the model has been optimized, evaluate the result on the test\n",
    "set.\n",
    "6. Extend the evaluation function so that it shows the Precision, Recall and F-score for each of\n",
    "the entity types (person, location, etc.) on the test set. Include the metrics for the B-label of\n",
    "the entity type, the I-label, and the full entities. Look up the definitions of macro- and micro-\n",
    "average scores and compute the macro- and micro average F1 scores over all entities.\n",
    "Write a report of at most 3 pages in which you:\n",
    "• describe the task and the data (give a few statistics. What are the entity types?) and briefly\n",
    "describe two challenges of the task and the data.\n",
    "• show your results:\n",
    "o a results table with both the baseline results and the results after hyperparameter\n",
    "optimization (do not report results on the dev set, only on the test set): a table with\n",
    "Precision, Recall, F-score for both settings.\n",
    "o a table with the results after hyperparameter optimization for the different entity\n",
    "types (Precision, Recall, F-score for B, I, and the full entities), and the macro- and\n",
    "micro F1 scores.\n",
    "• write brief conclusions. Address the following questions:\n",
    "o What is the effect of hyperparameter optimization on the quality of the model?\n",
    "o What does the difference between scores for different entity types tell you?\n",
    "o Where does the difference between macro- and micro-averaged F1 scores come\n",
    "from?\n",
    "Grading\n",
    "Maximum 2 points for each of the following criteria:\n",
    "• General: length correct (2-3 pages) and proper writing + formatting\n",
    "• Description of the task and the data, with description of 2 challenges\n",
    "• Baseline results with default hyperparameter settings and results with optimized\n",
    "hyperparameter settings\n",
    "• Results after hyperparameter optimization for the different entity types\n",
    "• Sensible conclusions, briefly addressing the questions listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from huggingface_hub import interpreter_login\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\n",
    "    \"O\",\n",
    "    \"B-corporation\",\n",
    "    \"I-corporation\",\n",
    "    \"B-creative-work\",\n",
    "    \"I-creative-work\",\n",
    "    \"B-group\",\n",
    "    \"I-group\",\n",
    "    \"B-location\",\n",
    "    \"I-location\",\n",
    "    \"B-person\",\n",
    "    \"I-person\",\n",
    "    \"B-product\",\n",
    "    \"I-product\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, valid, test = 'wnut17train.conll', 'emerging.dev.conll', 'emerging.test.annotated'\n",
    "path = 'W-NUT_data'\n",
    "\n",
    "raw_datasets = {}\n",
    "for file, name in zip([train, valid, test], ['train', 'validation', 'test']):\n",
    "    id = 0\n",
    "    raw_datasets[name] = {'id': [], 'tokens': [], 'ner_tags': []}\n",
    "    with open(f'{path}/{file}', 'r') as f:\n",
    "        tokens, ner_tags = [], []\n",
    "        for line in f:\n",
    "            try:\n",
    "                token, ner_tag = line.split()\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(names.index(ner_tag))\n",
    "            except:\n",
    "                raw_datasets[name]['id'].append([id for _ in range(len(tokens))])\n",
    "                raw_datasets[name]['tokens'].append(tokens)\n",
    "                raw_datasets[name]['ner_tags'].append(ner_tags)\n",
    "                id += 1\n",
    "                tokens, ner_tags = [], []\n",
    "    raw_datasets[name] = datasets.Dataset.from_dict(raw_datasets[name])\n",
    "\n",
    "raw_datasets = datasets.DatasetDict(raw_datasets)\n",
    "display(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c51419a1564b6789415ae0837a91c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867c453e45b14db48b65c85ddb1740a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6656cb187eb24e569b4d4165a205d837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/ruben/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()\n",
    "\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rubinho/bert-finetuned-ner/tree/main/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tex_table(all_metrics):\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\begin{tabular}{lrrrr}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\" & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1} & \\\\textbf{Number} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    for key, value in all_metrics.items():\n",
    "        if \"overall\" not in key:\n",
    "            print(f\"{key.title()} & {value['precision']:.2f} & {value['recall']:.2f} & {value['f1']:.2f} & {value['number']} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    print(\" & \\\\textbf{Overall precision} & \\\\textbf{Overall recall} & \\\\textbf{Overall F1} & \\\\textbf{Overall accuracy} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    print(f\" & {all_metrics['overall_precision']:.2f} & {all_metrics['overall_recall']:.2f} & {all_metrics['overall_f1']:.2f} & {all_metrics['overall_accuracy']:.2f} \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\caption{Results of the NER model on the test set.}\")\n",
    "    print(\"\\\\label{tab:ner-results}\")\n",
    "    print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1} & \\textbf{Number} \\\\\n",
      "\\midrule\n",
      "Corporation & 0.24 & 0.23 & 0.23 & 66 \\\\\n",
      "Creative-Work & 0.40 & 0.18 & 0.25 & 142 \\\\\n",
      "Group & 0.35 & 0.13 & 0.19 & 165 \\\\\n",
      "Location & 0.55 & 0.43 & 0.49 & 150 \\\\\n",
      "Person & 0.79 & 0.47 & 0.59 & 429 \\\\\n",
      "Product & 0.16 & 0.09 & 0.11 & 127 \\\\\n",
      "\\midrule\n",
      " & \\textbf{Overall precision} & \\textbf{Overall recall} & \\textbf{Overall F1} & \\textbf{Overall accuracy} \\\\\n",
      "\\midrule\n",
      " & 0.54 & 0.31 & 0.40 & 0.94 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Results of the NER model on the test set.}\n",
      "\\label{tab:ner-results}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "eval_preds = predictions.predictions, predictions.label_ids\n",
    "all_metrics = compute_all_metrics(eval_preds)\n",
    "print_tex_table(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptuna_hp_space\u001b[39m(trial):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: trial\u001b[39m.\u001b[39msuggest_float(\u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1e-6\u001b[39m, \u001b[39m1e-4\u001b[39m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m# \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True),\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     model_init\u001b[39m=\u001b[39moptuna_hp_space,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m best_trials \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mhyperparameter_search(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     backend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptuna\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     hp_space\u001b[39m=\u001b[39moptuna_hp_space,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     n_trials\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Documents/Studie/textmining/txtmining_a2/txtmin_a2.ipynb#X43sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "def model_init(): \n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "    \n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        # \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n",
    "        # \"beta_1\": trial.suggest_float(\"beta_1\", 0.9, 0.999),\n",
    "        # \"beta_2\": trial.suggest_float(\"beta_2\", 0.9, 0.999),\n",
    "        # \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        # \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
