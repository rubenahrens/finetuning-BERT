{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suzan Verberne 17.10.2023 1\n",
    "Assignment 2: sequence labelling\n",
    "Text mining course\n",
    "This is a hand-in assignment for groups of two students. Send in via Brightspace before or on\n",
    "Tuesday November 7:\n",
    "• Submit your report as PDF and your python code as separate file. Don’t upload a zip file\n",
    "containing the PDF (the Python code might be zipped if it consists of multiple files).\n",
    "• Your report should not be longer than 3 pages.\n",
    "• Do not copy text from sources (other groups, web pages, generative models such as\n",
    "chatGPT). Turnitin is enabled and a large overlap will be reported to the Board of Examiners.\n",
    "Goals of this assignment\n",
    "• You can pre-process existing annotated text data into the data structure that you need for\n",
    "classifier learning.\n",
    "• You can perform hyperparameter optimization.\n",
    "• You can perform a sequence labelling task with annotated data in Huggingface.\n",
    "• You can evaluate sequence labelling with the suitable evaluation metrics.\n",
    "Preliminaries\n",
    "• You have followed the Huggingface tutorial on token classification\n",
    "https://huggingface.co/learn/nlp-course/chapter7/2 and its preliminaries (exercises week 6\n",
    "and 7).\n",
    "• You have all the required Python packages installed and Python 3.10.\n",
    "We are going to train an NER classifier for the task “Emerging and Rare entity recognition” from the\n",
    "Workshop on Noisy User-generated Text (W-NUT). The description of the task can be found at\n",
    "https://noisy-text.github.io/2017/emerging-rare-entities.html (I put the data itself on Brightspace)\n",
    "Tasks\n",
    "1. Download W-NUT_data.zip from the Brightspace assignment and unzip the directory. It\n",
    "contains 3 IOB files: wnut17train.conll (train), emerging.dev.conll (dev),\n",
    "emerging.test.annotated (test).\n",
    "2. Convert the IOB data to the correct data structure for token classification in Huggingface\n",
    "(words and labels like the conll2023 data in the tutorial) and align the labels with the tokens.\n",
    "Note that since you are working with a custom dataset, the data conversion is a necessary\n",
    "step for using the Huggingface training function.\n",
    "Suzan Verberne 17.10.2023 2\n",
    "3. Set up the evaluation correctly for the W-NUT test set, following the tutorial.\n",
    "4. Fine-tune a model with the default hyperparameter settings on the train set and evaluate\n",
    "the model on the test set. These are your baseline results.\n",
    "5. Set up hyperparameter optimization with the AdamW optimizer. During optimization, use\n",
    "the dev set as validation. After the model has been optimized, evaluate the result on the test\n",
    "set.\n",
    "6. Extend the evaluation function so that it shows the Precision, Recall and F-score for each of\n",
    "the entity types (person, location, etc.) on the test set. Include the metrics for the B-label of\n",
    "the entity type, the I-label, and the full entities. Look up the definitions of macro- and micro-\n",
    "average scores and compute the macro- and micro average F1 scores over all entities.\n",
    "Write a report of at most 3 pages in which you:\n",
    "• describe the task and the data (give a few statistics. What are the entity types?) and briefly\n",
    "describe two challenges of the task and the data.\n",
    "• show your results:\n",
    "o a results table with both the baseline results and the results after hyperparameter\n",
    "optimization (do not report results on the dev set, only on the test set): a table with\n",
    "Precision, Recall, F-score for both settings.\n",
    "o a table with the results after hyperparameter optimization for the different entity\n",
    "types (Precision, Recall, F-score for B, I, and the full entities), and the macro- and\n",
    "micro F1 scores.\n",
    "• write brief conclusions. Address the following questions:\n",
    "o What is the effect of hyperparameter optimization on the quality of the model?\n",
    "o What does the difference between scores for different entity types tell you?\n",
    "o Where does the difference between macro- and micro-averaged F1 scores come\n",
    "from?\n",
    "Grading\n",
    "Maximum 2 points for each of the following criteria:\n",
    "• General: length correct (2-3 pages) and proper writing + formatting\n",
    "• Description of the task and the data, with description of 2 challenges\n",
    "• Baseline results with default hyperparameter settings and results with optimized\n",
    "hyperparameter settings\n",
    "• Results after hyperparameter optimization for the different entity types\n",
    "• Sensible conclusions, briefly addressing the questions listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from huggingface_hub import interpreter_login\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\n",
    "    \"O\",\n",
    "    \"B-corporation\",\n",
    "    \"I-corporation\",\n",
    "    \"B-creative-work\",\n",
    "    \"I-creative-work\",\n",
    "    \"B-group\",\n",
    "    \"I-group\",\n",
    "    \"B-location\",\n",
    "    \"I-location\",\n",
    "    \"B-person\",\n",
    "    \"I-person\",\n",
    "    \"B-product\",\n",
    "    \"I-product\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, valid, test = 'wnut17train.conll', 'emerging.dev.conll', 'emerging.test.annotated'\n",
    "path = 'W-NUT_data'\n",
    "\n",
    "raw_datasets = {}\n",
    "for file, name in zip([train, valid, test], ['train', 'validation', 'test']):\n",
    "    id = 0\n",
    "    raw_datasets[name] = {'id': [], 'tokens': [], 'ner_tags': []}\n",
    "    with open(f'{path}/{file}', 'r') as f:\n",
    "        tokens, ner_tags = [], []\n",
    "        for line in f:\n",
    "            try:\n",
    "                token, ner_tag = line.split()\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(names.index(ner_tag))\n",
    "            except:\n",
    "                raw_datasets[name]['id'].append([id for _ in range(len(tokens))])\n",
    "                raw_datasets[name]['tokens'].append(tokens)\n",
    "                raw_datasets[name]['ner_tags'].append(ner_tags)\n",
    "                id += 1\n",
    "                tokens, ner_tags = [], []\n",
    "    raw_datasets[name] = datasets.Dataset.from_dict(raw_datasets[name])\n",
    "\n",
    "raw_datasets = datasets.DatasetDict(raw_datasets)\n",
    "display(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b946ad8a068f45bcbfbd695dfc80dcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12af5818bf424089989cf23c2b411845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca99deea832c4a64b2f638b7e2ca3211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tex_table(all_metrics, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"\\\\begin{table}[h]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{lrrrr}\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\" & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1} & \\\\textbf{Number} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        for key, value in all_metrics.items():\n",
    "            if \"overall\" not in key:\n",
    "                f.write(f\"{key.title()} & {value['precision']:.2f} & {value['recall']:.2f} & {value['f1']:.2f} & {value['number']} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\" & \\\\textbf{Overall precision} & \\\\textbf{Overall recall} & \\\\textbf{Overall F1} & \\\\textbf{Overall accuracy} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(f\" & {all_metrics['overall_precision']:.2f} & {all_metrics['overall_recall']:.2f} & {all_metrics['overall_f1']:.2f} & {all_metrics['overall_accuracy']:.2f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "    \n",
    "def print_fancy_table(all_metrics):\n",
    "    # display fancy table in ipython widget\n",
    "    from IPython.display import display, HTML\n",
    "    html = \"<table>\"\n",
    "    html += \"<tr><th></th><th>Precision</th><th>Recall</th><th>F1</th><th>Number</th></tr>\"\n",
    "    for key, value in all_metrics.items():\n",
    "        if \"overall\" not in key:\n",
    "            html += f\"<tr><td>{key.title()}</td><td>{value['precision']:.2f}</td><td>{value['recall']:.2f}</td><td>{value['f1']:.2f}</td><td>{value['number']}</td></tr>\"\n",
    "    html += \"<tr><th></th><th>Overall precision</th><th>Overall recall</th><th>Overall F1</th><th>Overall accuracy</th></tr>\"\n",
    "    html += f\"<tr><td></td><td>{all_metrics['overall_precision']:.2f}</td><td>{all_metrics['overall_recall']:.2f}</td><td>{all_metrics['overall_f1']:.2f}</td><td>{all_metrics['overall_accuracy']:.2f}</td></tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Ruben\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca39101b380440d924efc527543f10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4a1dd02d7d43088b3a7d547271a040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3455404043197632, 'eval_precision': 0.6007751937984496, 'eval_recall': 0.3708133971291866, 'eval_f1': 0.45857988165680474, 'eval_accuracy': 0.9154937115333155, 'eval_runtime': 3.3283, 'eval_samples_per_second': 303.154, 'eval_steps_per_second': 38.157, 'epoch': 1.0}\n",
      "{'loss': 0.203, 'learning_rate': 1.215686274509804e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74442b197cdf4160991017c2866f04f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3773612380027771, 'eval_precision': 0.6176470588235294, 'eval_recall': 0.4019138755980861, 'eval_f1': 0.48695652173913045, 'eval_accuracy': 0.9206850414771207, 'eval_runtime': 3.4132, 'eval_samples_per_second': 295.62, 'eval_steps_per_second': 37.209, 'epoch': 2.0}\n",
      "{'loss': 0.0832, 'learning_rate': 4.313725490196079e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c887739d05da433a8218de3bdff8176e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3647298216819763, 'eval_precision': 0.5763688760806917, 'eval_recall': 0.4784688995215311, 'eval_f1': 0.5228758169934641, 'eval_accuracy': 0.926036928017126, 'eval_runtime': 3.5037, 'eval_samples_per_second': 287.977, 'eval_steps_per_second': 36.247, 'epoch': 3.0}\n",
      "{'train_runtime': 182.147, 'train_samples_per_second': 55.9, 'train_steps_per_second': 7.0, 'train_loss': 0.1257201654770795, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rubinho/bert-finetuned-ner/tree/main/'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b87337d18df47c883939345e9d06502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th></th><th>Precision</th><th>Recall</th><th>F1</th><th>Number</th></tr><tr><td>Corporation</td><td>0.15</td><td>0.15</td><td>0.15</td><td>66</td></tr><tr><td>Creative-Work</td><td>0.51</td><td>0.22</td><td>0.31</td><td>142</td></tr><tr><td>Group</td><td>0.35</td><td>0.15</td><td>0.21</td><td>165</td></tr><tr><td>Location</td><td>0.51</td><td>0.45</td><td>0.48</td><td>150</td></tr><tr><td>Person</td><td>0.75</td><td>0.44</td><td>0.56</td><td>429</td></tr><tr><td>Product</td><td>0.12</td><td>0.09</td><td>0.10</td><td>127</td></tr><tr><th></th><th>Overall precision</th><th>Overall recall</th><th>Overall F1</th><th>Overall accuracy</th></tr><tr><td></td><td>0.50</td><td>0.31</td><td>0.38</td><td>0.93</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "eval_preds = predictions.predictions, predictions.label_ids\n",
    "all_metrics = compute_all_metrics(eval_preds)\n",
    "write_tex_table(all_metrics, 'bert-finetuned-ner_baseline.txt')\n",
    "print_fancy_table(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2023-11-03 16:52:23,114] A new study created in memory with name: no-name-9fe29ea9-b5b6-4df8-9b5b-aec8cc8c2900\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274e4f92be504c499023d68d8b51c4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ded17dd241452892068441e52eb7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.718856930732727, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8816697886004816, 'eval_runtime': 3.5621, 'eval_samples_per_second': 283.257, 'eval_steps_per_second': 35.653, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b41870473d44f4aaf7fd6d72c6ca27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6257607936859131, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8816697886004816, 'eval_runtime': 3.537, 'eval_samples_per_second': 285.267, 'eval_steps_per_second': 35.906, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c18ad9106240239c1bbd258b9ebc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.598440945148468, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8816697886004816, 'eval_runtime': 5.4177, 'eval_samples_per_second': 186.24, 'eval_steps_per_second': 23.442, 'epoch': 3.0}\n",
      "{'train_runtime': 195.3068, 'train_samples_per_second': 52.133, 'train_steps_per_second': 1.644, 'train_loss': 0.4534582215306172, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-03 16:55:40,869] Trial 0 finished with value: 0.8816697886004816 and parameters: {'learning_rate': 3.6298652877725645e-06, 'per_device_train_batch_size': 32, 'beta_1': 0.984017712988234, 'beta_2': 0.9669996086698486, 'weight_decay': 0.054484136285551925, 'adam_epsilon': 1.3490158743843502e-09}. Best is trial 0 with value: 0.8816697886004816.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0b890c91054c7d93c5ed94a935cb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2571aa5e56434610b45acb8179f5d644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4627852141857147, 'eval_precision': 0.171875, 'eval_recall': 0.013157894736842105, 'eval_f1': 0.024444444444444446, 'eval_accuracy': 0.8852555525822853, 'eval_runtime': 3.3591, 'eval_samples_per_second': 300.381, 'eval_steps_per_second': 37.808, 'epoch': 1.0}\n",
      "{'loss': 0.3098, 'learning_rate': 3.1045635578406782e-06, 'epoch': 1.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a735324a026b47b8b92aaad2607c577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4367040693759918, 'eval_precision': 0.5683890577507599, 'eval_recall': 0.2236842105263158, 'eval_f1': 0.3210300429184549, 'eval_accuracy': 0.8995986085094996, 'eval_runtime': 3.4024, 'eval_samples_per_second': 296.557, 'eval_steps_per_second': 37.327, 'epoch': 2.0}\n",
      "{'loss': 0.1736, 'learning_rate': 1.1016193269757248e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ce93c1f82c49dc9884681395ce3d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4069167375564575, 'eval_precision': 0.5753138075313807, 'eval_recall': 0.32894736842105265, 'eval_f1': 0.4185692541856925, 'eval_accuracy': 0.9076799571849077, 'eval_runtime': 3.3404, 'eval_samples_per_second': 302.063, 'eval_steps_per_second': 38.02, 'epoch': 3.0}\n",
      "{'train_runtime': 180.0553, 'train_samples_per_second': 56.549, 'train_steps_per_second': 7.081, 'train_loss': 0.22230459475049785, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-03 17:00:00,142] Trial 1 finished with value: 2.230510387323034 and parameters: {'learning_rate': 5.107507788705632e-06, 'per_device_train_batch_size': 8, 'beta_1': 0.9509822023671163, 'beta_2': 0.947724818069466, 'weight_decay': 0.000454495444088876, 'adam_epsilon': 8.455546642454333e-08}. Best is trial 0 with value: 0.8816697886004816.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "def model_init(): \n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")    \n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"adam_beta1\": trial.suggest_float(\"beta_1\", 0.9, 0.999),\n",
    "        \"adam_beta2\": trial.suggest_float(\"beta_2\", 0.9, 0.999),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb95862602b4c43a2e123dcd3a94fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76079577620d4fa3b8d7ac647981ff99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3711134195327759, 'eval_precision': 0.5635135135135135, 'eval_recall': 0.4988038277511962, 'eval_f1': 0.5291878172588833, 'eval_accuracy': 0.9263580412095264, 'eval_runtime': 3.4128, 'eval_samples_per_second': 295.653, 'eval_steps_per_second': 37.213, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c0cad98b0e4755bb9695f2828320d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3694494962692261, 'eval_precision': 0.55, 'eval_recall': 0.4868421052631579, 'eval_f1': 0.516497461928934, 'eval_accuracy': 0.9256622959593257, 'eval_runtime': 3.4993, 'eval_samples_per_second': 288.347, 'eval_steps_per_second': 36.293, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ad23376e664f44b77ec0db7da6b23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3760727345943451, 'eval_precision': 0.559228650137741, 'eval_recall': 0.48564593301435405, 'eval_f1': 0.5198463508322664, 'eval_accuracy': 0.9257693336901258, 'eval_runtime': 3.5526, 'eval_samples_per_second': 284.015, 'eval_steps_per_second': 35.748, 'epoch': 3.0}\n",
      "{'train_runtime': 115.3098, 'train_samples_per_second': 88.301, 'train_steps_per_second': 2.784, 'train_loss': 0.036124984051951, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ce65ae455748beb3e452840c342860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rubinho/bert-finetuned-ner/tree/main/'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d83f495e924b2eb8e057d30b17b571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th></th><th>Precision</th><th>Recall</th><th>F1</th><th>Number</th></tr><tr><td>Corporation</td><td>0.16</td><td>0.20</td><td>0.18</td><td>66</td></tr><tr><td>Creative-Work</td><td>0.51</td><td>0.25</td><td>0.33</td><td>142</td></tr><tr><td>Group</td><td>0.37</td><td>0.16</td><td>0.23</td><td>165</td></tr><tr><td>Location</td><td>0.52</td><td>0.43</td><td>0.47</td><td>150</td></tr><tr><td>Person</td><td>0.77</td><td>0.46</td><td>0.57</td><td>429</td></tr><tr><td>Product</td><td>0.16</td><td>0.09</td><td>0.12</td><td>127</td></tr><tr><th></th><th>Overall precision</th><th>Overall recall</th><th>Overall F1</th><th>Overall accuracy</th></tr><tr><td></td><td>0.51</td><td>0.32</td><td>0.40</td><td>0.94</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "eval_preds = predictions.predictions, predictions.label_ids\n",
    "all_metrics = compute_all_metrics(eval_preds)\n",
    "write_tex_table(all_metrics, 'bert-finetuned-ner_optuna.txt')\n",
    "print_fancy_table(all_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
